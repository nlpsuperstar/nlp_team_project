{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84b35129-a8a1-4d53-b061-0a61be4b81aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/hector/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/hector/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "import unicodedata\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import Dict, List, Optional, Union, cast\n",
    "from time import strftime, sleep\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import requests\n",
    "from requests import get\n",
    "\n",
    "from prepare import *\n",
    "from acquire import *\n",
    "\n",
    "from env import github_token, github_username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc361247-0e52-416b-bc64-c8db8be5a463",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#run acquire.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0532e2d1-cf50-4b5b-b645-10971b5a131b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo</th>\n",
       "      <th>language</th>\n",
       "      <th>readme_contents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>thedaviddias/Front-End-Checklist</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;h1 align=\"center\"&gt;\\n&lt;br&gt;\\n  &lt;img src=\"https:/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>onevcat/Kingfisher</td>\n",
       "      <td>Swift</td>\n",
       "      <td>&lt;p align=\"center\"&gt;\\n&lt;img src=\"https://raw.gith...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FallibleInc/security-guide-for-developers</td>\n",
       "      <td>None</td>\n",
       "      <td># 实用性 WEB 开发人员安全须知  \\n\\n### 目标读者  \\n\\n安全问题主要由以...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tailwindlabs/tailwindcss</td>\n",
       "      <td>JavaScript</td>\n",
       "      <td>&lt;p&gt;\\n    &lt;a href=\"https://tailwindcss.com/\" ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>codepath/android_guides</td>\n",
       "      <td>None</td>\n",
       "      <td># CodePath Android Cliffnotes\\n\\nWelcome to th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        repo    language  \\\n",
       "0           thedaviddias/Front-End-Checklist        None   \n",
       "1                         onevcat/Kingfisher       Swift   \n",
       "2  FallibleInc/security-guide-for-developers        None   \n",
       "3                   tailwindlabs/tailwindcss  JavaScript   \n",
       "4                    codepath/android_guides        None   \n",
       "\n",
       "                                     readme_contents  \n",
       "0  <h1 align=\"center\">\\n<br>\\n  <img src=\"https:/...  \n",
       "1  <p align=\"center\">\\n<img src=\"https://raw.gith...  \n",
       "2  # 实用性 WEB 开发人员安全须知  \\n\\n### 目标读者  \\n\\n安全问题主要由以...  \n",
       "3  <p>\\n    <a href=\"https://tailwindcss.com/\" ta...  \n",
       "4  # CodePath Android Cliffnotes\\n\\nWelcome to th...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reads in json\n",
    "df = pd.read_json('data.json')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80ba614d-4a64-4880-93eb-01880eedc141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(560, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14aabf61-ca41-49a3-b01e-8394730d1eed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JavaScript          141\n",
       "Python               59\n",
       "NaN                  54\n",
       "Java                 46\n",
       "TypeScript           45\n",
       "Go                   36\n",
       "C++                  30\n",
       "C                    20\n",
       "Swift                13\n",
       "Shell                13\n",
       "HTML                 12\n",
       "Rust                 11\n",
       "Kotlin                9\n",
       "C#                    9\n",
       "Ruby                  9\n",
       "PHP                   6\n",
       "Jupyter Notebook      6\n",
       "Vue                   6\n",
       "Vim script            6\n",
       "CSS                   5\n",
       "Objective-C           3\n",
       "Haskell               2\n",
       "CoffeeScript          2\n",
       "Elixir                2\n",
       "TeX                   2\n",
       "Lua                   2\n",
       "Batchfile             1\n",
       "Standard ML           1\n",
       "Emacs Lisp            1\n",
       "Makefile              1\n",
       "Crystal               1\n",
       "Dockerfile            1\n",
       "OCaml                 1\n",
       "Clojure               1\n",
       "Rascal                1\n",
       "Dart                  1\n",
       "AsciiDoc              1\n",
       "Name: language, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.language.value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f820f86-592a-41cf-b77c-30c028b3aa75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "repo                0\n",
       "language           54\n",
       "readme_contents     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sums null counts\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd866ecd-a0db-418e-9d8c-9cbac25de393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#drops nulls\n",
    "df = df.dropna()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d45d569-c40f-4a94-af9c-984bdf989874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo</th>\n",
       "      <th>language</th>\n",
       "      <th>readme_contents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>onevcat/Kingfisher</td>\n",
       "      <td>Swift</td>\n",
       "      <td>&lt;p align=\"center\"&gt;\\n&lt;img src=\"https://raw.gith...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tailwindlabs/tailwindcss</td>\n",
       "      <td>JavaScript</td>\n",
       "      <td>&lt;p&gt;\\n    &lt;a href=\"https://tailwindcss.com/\" ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>github/fetch</td>\n",
       "      <td>JavaScript</td>\n",
       "      <td># window.fetch polyfill\\n\\nThe `fetch()` funct...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ianstormtaylor/slate</td>\n",
       "      <td>TypeScript</td>\n",
       "      <td>&lt;p align=\"center\"&gt;\\n  &lt;a href=\"#\"&gt;&lt;img src=\"./...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kong/insomnia</td>\n",
       "      <td>JavaScript</td>\n",
       "      <td># Insomnia REST Client\\n\\n[![Slack Channel](ht...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       repo    language  \\\n",
       "0        onevcat/Kingfisher       Swift   \n",
       "1  tailwindlabs/tailwindcss  JavaScript   \n",
       "2              github/fetch  JavaScript   \n",
       "3      ianstormtaylor/slate  TypeScript   \n",
       "4             Kong/insomnia  JavaScript   \n",
       "\n",
       "                                     readme_contents  \n",
       "0  <p align=\"center\">\\n<img src=\"https://raw.gith...  \n",
       "1  <p>\\n    <a href=\"https://tailwindcss.com/\" ta...  \n",
       "2  # window.fetch polyfill\\n\\nThe `fetch()` funct...  \n",
       "3  <p align=\"center\">\\n  <a href=\"#\"><img src=\"./...  \n",
       "4  # Insomnia REST Client\\n\\n[![Slack Channel](ht...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reset index \n",
    "df = df.reset_index(drop = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e3504b-b1d4-4f52-a46d-5660ed622107",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#functions used in our current work.\n",
    "#pulled it out to work with it.\n",
    "def basic_clean(string):\n",
    "    '''\n",
    "    This function takes in a string and normalizes it for nlp purposes\n",
    "    '''\n",
    "    # lowercase the string\n",
    "    string = string.lower()\n",
    "\n",
    "    # return normal form for the unicode string, encode/remove ascii\n",
    "    string = unicodedata.normalize('NFKD', string).encode('ascii', 'ignore').decode('utf-8')\n",
    "    \n",
    "    # breaks down the string by keeping alphabet letters, numbers, apostraphes and spaces\n",
    "    string = re.sub(r\"[^a-z0-9\\s]\", '', string)\n",
    "    \n",
    "    return string\n",
    "\n",
    "\n",
    "def tokenize(string):\n",
    "    '''\n",
    "    This function takes in a string and tokenizes it\n",
    "    '''\n",
    "    # create the tokenizer\n",
    "    tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "    \n",
    "    # use the tokenizer, return as a string\n",
    "    string = tokenizer.tokenize(string, return_str = True)\n",
    "    \n",
    "    return string\n",
    "\n",
    "def stem(text):\n",
    "    '''\n",
    "    This function takes in a text and stems the words to their original stem\n",
    "    '''\n",
    "    \n",
    "    # create a porter stemmer\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    \n",
    "    # loop through the text to stem the words\n",
    "    stems = [ps.stem(word) for word in text.split()]\n",
    "    \n",
    "    # return back together\n",
    "    stems = ' '.join(stems)\n",
    "    \n",
    "    return stems\n",
    "\n",
    "\n",
    "def lemmatize(text):\n",
    "    '''\n",
    "    This function takes in a text and changes the words back to their root (lemmatize)\n",
    "    '''\n",
    "    \n",
    "    # create the lemmatizer\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    \n",
    "    # loop through the list to split and lemmatize\n",
    "    lemmas = [wnl.lemmatize(word) for word in text.split()]\n",
    "    \n",
    "    # return back together\n",
    "    lemmas =' '.join(lemmas)\n",
    "    \n",
    "    return lemmas\n",
    "\n",
    "\n",
    "def remove_stopwords(string, extra_words = [], exclude_words = []):\n",
    "    '''\n",
    "    This function takes in a string\n",
    "    And returns the string with the English stopwords removed\n",
    "    Additional stopwords can be added to extra_words (list)\n",
    "    or words to exclude from stopwords can be added to exclude_words (list)\n",
    "    \n",
    "    -- This might break if the excluded words aren't in the stopwords list\n",
    "    '''\n",
    "    # define stopwords list      \n",
    "    stopwords_list = stopwords.words('English')\n",
    "    \n",
    "    # add or remove words based on arguments\n",
    "    stopwords_list = set(stopwords_list) - set(exclude_words) # the set removes words\n",
    "    \n",
    "    stopwords_list = stopwords_list.union(set(extra_words))\n",
    "        \n",
    "    # remove stopwords from string\n",
    "    # turn string into list\n",
    "    words = string.split()\n",
    "    \n",
    "    # remove the stopwords\n",
    "    filtered_words = [w for w in words if w not in stopwords_list]\n",
    "    \n",
    "    # turn back into a string\n",
    "    new_string = ' '.join(filtered_words)\n",
    "    \n",
    "    return new_string\n",
    "\n",
    "\n",
    "################## ~~~~~~ Mother Prep Function ~~~~~~ ##################\n",
    "\n",
    "def prepare_nlp_data(df, content = 'content', extra_words=[], exclude_words=[]):\n",
    "    '''\n",
    "    This function take in a df and the content (in string) for the column \n",
    "    with an option to pass lists for additional stopwords (extra_words)\n",
    "    and an option to pass words to exclude from stopwords (exclude words)\n",
    "    returns a df with the  original text, cleaned (tokenized and stopwords removed),\n",
    "    stemmed text, lemmatized text.\n",
    "    '''\n",
    "    df['clean'] = df[content].apply(basic_clean)\\\n",
    "                            .apply(tokenize)\\\n",
    "                            .apply(remove_stopwords, \n",
    "                                   extra_words=extra_words, exclude_words=exclude_words)\n",
    "    \n",
    "    df['stemmed'] = df['clean'].apply(stem)\n",
    "    \n",
    "    df['lemmatized'] = df['clean'].apply(lemmatize)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def is_chinese(texts):\n",
    "    '''\n",
    "    This function takes in a dataframe and return true if the scanned text is in chinese\n",
    "    '''\n",
    "    if re.search(\"[\\u4e00-\\u9FFF]\", texts):\n",
    "            return True\n",
    "\n",
    "\n",
    "\n",
    "def get_top_4_languages(df):\n",
    "    '''\n",
    "    This function takes in a dataframe and returns the top four\n",
    "    programming languages found in the data\n",
    "    '''\n",
    "    top_4_list = list(df.language.value_counts().head(4).index)\n",
    "    mask = df.language.apply(lambda x: x in top_4_list)\n",
    "    df = df[mask]\n",
    "    return df\n",
    "\n",
    "\n",
    "def drop_unneeded_data(df):\n",
    "    '''\n",
    "    This function takes in the repo dataframe\n",
    "    Drops any rows with nulls\n",
    "    Drops any rows that are chinese\n",
    "    Drops all rows that aren't in the top 4 languages\n",
    "    '''\n",
    "    df = df.dropna()\n",
    "    df = df[df.readme_contents.apply(is_chinese) !=True]\n",
    "    df = get_top_4_languages(df)\n",
    "    df = df.reset_index().drop(columns = 'index')\n",
    "    return df\n",
    "\n",
    "\n",
    "def split_data(df):\n",
    "    '''\n",
    "    This function takes in a dataframe and splits it into train, test, and \n",
    "    validate dataframes for my model\n",
    "    '''\n",
    "\n",
    "    train_validate, test = train_test_split(df, test_size=.2, \n",
    "                                        random_state=123, stratify=df.language)\n",
    "    train, validate = train_test_split(train_validate, test_size=.3, \n",
    "                                   random_state=123, stratify=train_validate.language)\n",
    "\n",
    "    print('train--->', train.shape)\n",
    "    print('validate--->', validate.shape)\n",
    "    print('test--->', test.shape)\n",
    "    return train, validate, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df3a8166-58b7-4f53-81b2-75d41b6c59fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this takes in the readme_content and prepares it.\n",
    "#the result is a clean value\n",
    "text = df['readme_contents'].iloc[0] #takes in the first value of the 1st row of the readme_contents column and assigns it 'text' variable\n",
    "text = text.lower()#takes in the variable and sets all to lower\n",
    "soup = BeautifulSoup(text, 'html.parser')#soups it!\n",
    "text = soup.get_text()#grabs the texts from the readme_contents\n",
    "text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')#normalized and encodes\n",
    "text = re.sub(r\"[^a-z0-9'\\s]\", '', text)#keeps alpha numeric characters\n",
    "wnl = nltk.stem.WordNetLemmatizer()#lemma\n",
    "lemmas = [wnl.lemmatize(word) for word in text.split()]#lemma loop\n",
    "text_lemma = ' '.join(lemmas)#lemmas\n",
    "stopwords = nltk.corpus.stopwords.words('english')#stopword\n",
    "newStopWords = ['u','ha','wa']#we can change these. I left it in from my last exercise as a place holder if we found any other stopwords we wanted to use.\n",
    "stopwords.extend(newStopWords)#adds new stopwords\n",
    "words = text_lemma.split()#splits\n",
    "filtered_words = [w for w in words if w not in stopwords]#loops the split\n",
    "speech = ' '.join(filtered_words)#joins it all thgether\n",
    "\n",
    "#Could not figure out how to loop this correctly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00bb922c-853d-4de7-ba46-dadb5bdd9966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"kingfisher powerful pureswift library downloading caching image web provides chance use pureswift way work remote image next app feature x asynchronous image downloading caching x loading image either urlsessionbased networking local provided data x useful image processor filter provided x multiplelayer hybrid cache memory disk x fine control cache behavior customizable expiration date size limit x cancelable downloading autoreusing previous downloaded content improve performance x independent component use downloader caching system image processor separately need x prefetching image showing cache boost app x view extension uiimageview nsimageview nsbutton uibutton directly set image url x builtin transition animation setting image x customizable placeholder indicator loading image x extensible image processing image format easily x low data mode support x swiftui support kingfisher 101 simplest usecase setting image image view uiimageview extension swift import kingfisher let url urlstring httpsexamplecomimagepng imageviewkfsetimagewith url kingfisher download image url send memory cache disk cache display imageview set url later image retrieved cache shown immediately also work use swiftui swift var body view kfimageurlstring httpsexamplecomimagepng advanced example powerful option hard task kingfisher simple way example code 1 downloads highresolution image 2 downsamples match image view size 3 make round cornered given radius 4 show system indicator placeholder image downloading 5 prepared animates small thumbnail image fade effect 6 original large image also cached disk later use get rid downloading detail view 7 console log printed task finish either success failure swift let url urlstring httpsexamplecomhighresolutionimagepng let processor downsamplingimageprocessorsize imageviewboundssize roundcornerimageprocessorcornerradius 20 imageviewkfindicatortype activity imageviewkfsetimage url placeholder uiimagenamed placeholderimage option processorprocessor scalefactoruiscreenmainscale transitionfade1 cacheoriginalimage result switch result case successlet value printtask done valuesourceurlabsolutestring case failurelet error printjob failed errorlocalizeddescription common situation meet daily work think many line need write without kingfisher method chaining fan kf extension also prefer use kf builder chained method invocation code thing swift use kf extension imageviewkfsetimage url placeholder placeholderimage option processorprocessor loaddiskfilesynchronously cacheoriginalimage transitionfade025 lowdatamodenetworklowresolutionurl progressblock receivedsize totalsize progress updated completionhandler result done use kf builder kfurlurl placeholderplaceholderimage setprocessorprocessor loaddiskfilesynchronously cachememoryonly fadeduration 025 lowdatamodesourcenetworklowresolutionurl onprogress receivedsize totalsize onsuccess result onfailure error setto imageview even better later want switch swiftui change kf kfimage done swift struct contentview view var body view kfimageurlurl placeholderplaceholderimage setprocessorprocessor loaddiskfilesynchronously cachememoryonly fadeduration 025 lowdatamodesourcenetworklowresolutionurl onprogress receivedsize totalsize onsuccess result onfailure error learn learn use kingfisher example take look wellprepared cheat sheethttpsgithubcomonevcatkingfisherwikicheatsheet summarized common task kingfisher get better idea framework also performance tip remember check requirement io 120 macos 1014 tvos 120 watchos 50 use uikitappkit io 140 macos 110 tvos 140 watchos 70 use swiftui swift 50 need support io 10 uikitappkit io 13 swiftui use kingfisher version 6x work xcode 13 1802httpsgithubcomonevcatkingfisherissues1802 need use xcode 13 cannot upgrade v7 use version6xcode13 branch however drop io 10 support due xcode 13 bug uikit swiftui xcode kingfisher io 10 io 13 12 631 io 11 io 13 13 version6xcode13 io 12 io 14 13 70 installation detailed guide installation found installation guidehttpsgithubcomonevcatkingfisherwikiinstallationguide swift package manager file swift package add package dependency add httpsgithubcomonevcatkingfishergit select next major 700 cocoapods ruby source 'httpsgithubcomcocoapodsspecsgit' platform io '120' useframeworks target 'myapp' pod 'kingfisher' ' 70' end carthage github onevcatkingfisher 70 migrating kingfisher 70 migrationhttpsgithubcomonevcatkingfisherwikikingfisher70migrationguide kingfisher 7x fully compatible previous version however change trivial required please follow migration guidehttpsgithubcomonevcatkingfisherwikikingfisher70migrationguide prepare upgrade kingfisher project using even earlier version see guide know step migrating kingfisher 60 migrationhttpsgithubcomonevcatkingfisherwikikingfisher60migrationguide kingfisher 6x fully compatible previous version however migration difficult depending use case may take effect several minute modify existing code new version please follow migration guidehttpsgithubcomonevcatkingfisherwikikingfisher60migrationguide prepare upgrade kingfisher project kingfisher 50 migrationhttpsgithubcomonevcatkingfisherwikikingfisher50migrationguide upgrading kingfisher 5x 4x please read information kingfisher 40 migration kingfisher 3x source compatible kingfisher 4 reason major update need specify swift version explicitly xcode deprecated method kingfisher 3 removed please ensure warning left migrate kingfisher 3 kingfisher 4 trouble migrating please open issue discus kingfisher 30 migrationhttpsgithubcomonevcatkingfisherwikikingfisher30migrationguide upgrading kingfisher 3x earlier version please read information next step prepared wiki pagehttpsgithubcomonevcatkingfisherwiki find ton useful thing installation guidehttpsgithubcomonevcatkingfisherwikiinstallationguide follow integrate kingfisher project cheat sheethttpsgithubcomonevcatkingfisherwikicheatsheet curious kingfisher could would look like used project see page useful code snippet already familiar kingfisher could also learn new trick improve way use kingfisher api referencehttpskingfisheronevcatcom lastly please remember read full whenever may need detailed reference future kingfisher want keep kingfisher lightweight framework focus providing simple solution downloading caching image doesnt mean framework cant improved kingfisher far perfect necessary useful update made make better development test contributing pull request warmly welcome however plan implement feature try fix uncertain issue recommended open discussion first would appreciated pull request could build test green logo logo kingfisher inspired tangram httpenwikipediaorgwikitangram dissection puzzle consisting seven flat shape china believe kingfisher bird instead swift someone insists pigeon guess give name hi guy suggestion contact follow contact twitterhttptwittercomonevcat sina weibohttpweibocomonevcat find issue open tickethttpsgithubcomonevcatkingfisherissuesnew pull request warmly welcome well backer sponsor opensource project cannot live long without help find kingfisher useful please consider supporting project becoming sponsor user icon company logo show bloghttpsonevcatcomtabsabout link home page become sponsor github sponsorshttpsgithubcomsponsorsonevcat open collectivehttpsopencollectivecomkingfishersponsor heart license kingfisher released mit license see license detail\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#run after the above.\n",
    "#results == cleaned and prepared content\n",
    "speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32e60c5c-15f2-4a45-bec1-eb2530f7e258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo</th>\n",
       "      <th>language</th>\n",
       "      <th>readme_contents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>onevcat/Kingfisher</td>\n",
       "      <td>Swift</td>\n",
       "      <td>&lt;p align=\"center\"&gt;\\n&lt;img src=\"https://raw.gith...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tailwindlabs/tailwindcss</td>\n",
       "      <td>JavaScript</td>\n",
       "      <td>&lt;p&gt;\\n    &lt;a href=\"https://tailwindcss.com/\" ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>github/fetch</td>\n",
       "      <td>JavaScript</td>\n",
       "      <td># window.fetch polyfill\\n\\nThe `fetch()` funct...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ianstormtaylor/slate</td>\n",
       "      <td>TypeScript</td>\n",
       "      <td>&lt;p align=\"center\"&gt;\\n  &lt;a href=\"#\"&gt;&lt;img src=\"./...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kong/insomnia</td>\n",
       "      <td>JavaScript</td>\n",
       "      <td># Insomnia REST Client\\n\\n[![Slack Channel](ht...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       repo    language  \\\n",
       "0        onevcat/Kingfisher       Swift   \n",
       "1  tailwindlabs/tailwindcss  JavaScript   \n",
       "2              github/fetch  JavaScript   \n",
       "3      ianstormtaylor/slate  TypeScript   \n",
       "4             Kong/insomnia  JavaScript   \n",
       "\n",
       "                                     readme_contents  \n",
       "0  <p align=\"center\">\\n<img src=\"https://raw.gith...  \n",
       "1  <p>\\n    <a href=\"https://tailwindcss.com/\" ta...  \n",
       "2  # window.fetch polyfill\\n\\nThe `fetch()` funct...  \n",
       "3  <p align=\"center\">\\n  <a href=\"#\"><img src=\"./...  \n",
       "4  # Insomnia REST Client\\n\\n[![Slack Channel](ht...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4395065-930d-43ae-afba-63e206912aa8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#i was messing around with this, not sure if it helps.\n",
    "#it pulls ALL the text from every DOCUMENT...the 500+ readme_contents\n",
    "our_list = df['clean'].to_list()\n",
    "our_lish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294e06c6-0697-45f9-8156-79bf8b7d06f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7de38a-32c2-4148-aed0-5b90abe668b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Notes below. Some possible useful code to get rid of the chinese documents below...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4c2bf0-6e80-4c76-ac03-4ec32baea6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df['readme_contents'].iloc[0] \n",
    "text = text.lower()\n",
    "soup = BeautifulSoup(text, 'html.parser')\n",
    "text = soup.get_text()\n",
    "text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "text = re.sub(r\"[^a-z0-9'\\s]\", '', text)\n",
    "wnl = nltk.stem.WordNetLemmatizer()\n",
    "lemmas = [wnl.lemmatize(word) for word in text.split()]\n",
    "text_lemma = ' '.join(lemmas)\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "newStopWords = ['u','ha','wa']\n",
    "stopwords.extend(newStopWords)\n",
    "words = text_lemma.split()\n",
    "filtered_words = [w for w in words if w not in stopwords]\n",
    "speech = ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582c1ffb-ef91-4358-adfc-0103f1ca94b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d309c6-b294-4456-a5f8-14c80a883624",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6683480b-203e-4c6e-944e-fc30883c80c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "url = 'https://github.com/hoppscotch/hoppscotch'\n",
    "def stuff(url):\n",
    "    response = get(url, headers = {'User-Agent': 'Codeup Data Science'})\n",
    "    soup = BeautifulSoup(response.text, features='lxml')\n",
    "    speech = soup.select('.markdown-body')\n",
    "    speech = speech[0].select('p')\n",
    "    \n",
    "    words = [words.text for words in speech]\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0273dab3-f11b-4b38-ba29-8376c8de5b09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "words = stuff(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076f3058-c8d9-4737-80a4-9d4cc8da1e27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3383bf4-19f3-49d0-b444-b4289f060ed5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_speech(words):\n",
    "    original = ' '.join(words)\n",
    "    text = original.lower()\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    text = re.sub(r\"[^a-z0-9'\\s]\", '', text)\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    lemmas = [wnl.lemmatize(word) for word in text.split()]\n",
    "    text_lemma = ' '.join(lemmas)\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    newStopWords = ['u','ha','wa']\n",
    "    stopwords.extend(newStopWords)\n",
    "    words = text_lemma.split()\n",
    "    filtered_words = [w for w in words if w not in stopwords]\n",
    "    speech = ' '.join(filtered_words)\n",
    "\n",
    "    return speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01757380-c998-4391-a949-df1d5cc8ddb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "speech = clean_speech(words)\n",
    "speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bac02b3-0c5b-446b-8650-02ecd43605af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This gets rid of the chinese documents\n",
    "def is_chinese(texts):\n",
    "    '''\n",
    "    This function takes in a dataframe and return true if the scanned text is in chinese\n",
    "    '''\n",
    "    if re.search(\"[\\u4e00-\\u9FFF]\", texts):\n",
    "            return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c541063-2df2-4f7f-828e-f0179dbf5202",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drops true values from is_chinese function\n",
    "df = df[df.readme_contents.apply(is_chinese) !=True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2887dd4e-8068-404b-b666-7820b9b3fe0b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def basic_clean(string):\n",
    "    '''\n",
    "    This function takes in a string and normalizes it for nlp purposes\n",
    "    '''\n",
    "    # lowercase the string\n",
    "    string = string.lower()\n",
    "\n",
    "    # return normal form for the unicode string, encode/remove ascii\n",
    "    string = unicodedata.normalize('NFKD', string).encode('ascii', 'ignore').decode('utf-8')\n",
    "    \n",
    "    # breaks down the string by keeping alphabet letters, numbers, apostraphes and spaces\n",
    "    string = re.sub(r\"[^a-z0-9\\s]\", '', string)\n",
    "    \n",
    "    return string\n",
    "\n",
    "\n",
    "def tokenize(string):\n",
    "    '''\n",
    "    This function takes in a string and tokenizes it\n",
    "    '''\n",
    "    # create the tokenizer\n",
    "    tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "    \n",
    "    # use the tokenizer, return as a string\n",
    "    string = tokenizer.tokenize(string, return_str = True)\n",
    "    \n",
    "    return string\n",
    "\n",
    "def stem(text):\n",
    "    '''\n",
    "    This function takes in a text and stems the words to their original stem\n",
    "    '''\n",
    "    \n",
    "    # create a porter stemmer\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    \n",
    "    # loop through the text to stem the words\n",
    "    stems = [ps.stem(word) for word in text.split()]\n",
    "    \n",
    "    # return back together\n",
    "    stems = ' '.join(stems)\n",
    "    \n",
    "    return stems\n",
    "\n",
    "\n",
    "def lemmatize(text):\n",
    "    '''\n",
    "    This function takes in a text and changes the words back to their root (lemmatize)\n",
    "    '''\n",
    "    \n",
    "    # create the lemmatizer\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    \n",
    "    # loop through the list to split and lemmatize\n",
    "    lemmas = [wnl.lemmatize(word) for word in text.split()]\n",
    "    \n",
    "    # return back together\n",
    "    lemmas =' '.join(lemmas)\n",
    "    \n",
    "    return lemmas\n",
    "\n",
    "\n",
    "def remove_stopwords(string, extra_words = [], exclude_words = []):\n",
    "    '''\n",
    "    This function takes in a string\n",
    "    And returns the string with the English stopwords removed\n",
    "    Additional stopwords can be added to extra_words (list)\n",
    "    or words to exclude from stopwords can be added to exclude_words (list)\n",
    "    \n",
    "    -- This might break if the excluded words aren't in the stopwords list\n",
    "    '''\n",
    "    # define stopwords list      \n",
    "    stopwords_list = stopwords.words('English')\n",
    "    \n",
    "    # add or remove words based on arguments\n",
    "    stopwords_list = set(stopwords_list) - set(exclude_words) # the set removes words\n",
    "    \n",
    "    stopwords_list = stopwords_list.union(set(extra_words))\n",
    "        \n",
    "    # remove stopwords from string\n",
    "    # turn string into list\n",
    "    words = string.split()\n",
    "    \n",
    "    # remove the stopwords\n",
    "    filtered_words = [w for w in words if w not in stopwords_list]\n",
    "    \n",
    "    # turn back into a string\n",
    "    new_string = ' '.join(filtered_words)\n",
    "    \n",
    "    return new_string\n",
    "\n",
    "\n",
    "################## ~~~~~~ Mother Prep Function ~~~~~~ ##################\n",
    "\n",
    "def prepare_nlp_data(df, content = 'content', extra_words=[], exclude_words=[]):\n",
    "    '''\n",
    "    This function take in a df and the content (in string) for the column \n",
    "    with an option to pass lists for additional stopwords (extra_words)\n",
    "    and an option to pass words to exclude from stopwords (exclude words)\n",
    "    returns a df with the  original text, cleaned (tokenized and stopwords removed),\n",
    "    stemmed text, lemmatized text.\n",
    "    '''\n",
    "    df['clean'] = df[content].apply(basic_clean)\\\n",
    "                            .apply(tokenize)\\\n",
    "                            .apply(remove_stopwords, \n",
    "                                   extra_words=extra_words, exclude_words=exclude_words)\n",
    "    \n",
    "    df['stemmed'] = df['clean'].apply(stem)\n",
    "    \n",
    "    df['lemmatized'] = df['clean'].apply(lemmatize)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def is_chinese(texts):\n",
    "    '''\n",
    "    This function takes in a dataframe and return true if the scanned text is in chinese\n",
    "    '''\n",
    "    if re.search(\"[\\u4e00-\\u9FFF]\", texts):\n",
    "            return True\n",
    "\n",
    "\n",
    "\n",
    "def get_top_4_languages(df):\n",
    "    '''\n",
    "    This function takes in a dataframe and returns the top four\n",
    "    programming languages found in the data\n",
    "    '''\n",
    "    top_4_list = list(df.language.value_counts().head(4).index)\n",
    "    mask = df.language.apply(lambda x: x in top_4_list)\n",
    "    df = df[mask]\n",
    "    return df\n",
    "\n",
    "\n",
    "def drop_unneeded_data(df):\n",
    "    '''\n",
    "    This function takes in the repo dataframe\n",
    "    Drops any rows with nulls\n",
    "    Drops any rows that are chinese\n",
    "    Drops all rows that aren't in the top 4 languages\n",
    "    '''\n",
    "    df = df.dropna()\n",
    "    df = df[df.readme_contents.apply(is_chinese) !=True]\n",
    "    df = get_top_4_languages(df)\n",
    "    df = df.reset_index().drop(columns = 'index')\n",
    "    return df\n",
    "\n",
    "\n",
    "def split_data(df):\n",
    "    '''\n",
    "    This function takes in a dataframe and splits it into train, test, and \n",
    "    validate dataframes for my model\n",
    "    '''\n",
    "\n",
    "    train_validate, test = train_test_split(df, test_size=.2, \n",
    "                                        random_state=123, stratify=df.language)\n",
    "    train, validate = train_test_split(train_validate, test_size=.3, \n",
    "                                   random_state=123, stratify=train_validate.language)\n",
    "\n",
    "    print('train--->', train.shape)\n",
    "    print('validate--->', validate.shape)\n",
    "    print('test--->', test.shape)\n",
    "    return train, validate, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9486baf0-906e-4842-9b11-d46f63fce4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = prepare_nlp_data(df, content = 'readme_contents', extra_words=['customer', 'customers', '1', '2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62478439-b953-4518-9391-c4008edbb581",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eddab41-0c41-4c2d-a359-9dd03825e5be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4d59db2-a6f3-49bc-b273-dc767f4c2107",
   "metadata": {},
   "source": [
    "#### prep function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1802ae6a-78a3-4c9d-a442-f18fcaf2d4db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prep():\n",
    "#drop nulls\n",
    "    df = df.dropna()\n",
    "#drop written chinese\n",
    "    df = df[df.readme_contents.apply(is_chinese) !=True]\n",
    "#Keeps top 5 languages\n",
    "    df = top_5\n",
    "#reindex and drop old index column\n",
    "    df = df.reset_index(drop = True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa67c7d-43dc-4d75-bd15-89b179f570b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5289e1-c125-4add-8c64-b65500bd5181",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566d66ed-f37d-4fb8-a590-a66e5681e0be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "text = re.sub(r\"[^a-z0-9'\\s]\", '', text)\n",
    "wnl = nltk.stem.WordNetLemmatizer()\n",
    "lemmas = [wnl.lemmatize(word) for word in text.split()]\n",
    "text_lemma = ' '.join(lemmas)\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "newStopWords = ['u','ha','wa']\n",
    "stopwords.extend(newStopWords)\n",
    "words = text_lemma.split()\n",
    "filtered_words = [w for w in words if w not in stopwords]\n",
    "speech = ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa4145d-c143-430f-b80b-21305c39838f",
   "metadata": {},
   "outputs": [],
   "source": [
    "speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c29c872-dc60-4c55-a8a1-b495e34e3c1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
